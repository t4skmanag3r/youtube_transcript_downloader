{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': {'client': {'userAgent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36 OPR/83.0.4254.70,gzip(gfe)',\n",
       "   'clientName': 'WEB',\n",
       "   'clientVersion': '2.20220309.01.00'}},\n",
       " 'params': 'CgttZkdlY09hVHZnQQ%3D%3F'}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can drop everything except: \n",
    "# {\"context\":{\"client\":{\"userAgent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36 OPR/83.0.4254.70,gzip(gfe)\",\n",
    "# \"clientName\":\"WEB\",\n",
    "# \"clientVersion\":\"2.20220309.01.00\"}},\n",
    "# \"params\":\"CgttZkdlY09hVHZnQQ%3D%3D\"}\n",
    "with open(\"transcript_payload_short.txt\") as f: \n",
    "    payload = json.loads(f.read())\n",
    "\n",
    "payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_url = \"https://www.youtube.com/youtubei/v1/get_transcript?key=AIzaSyAO_FJ2SlqU8Q4STEHLGCilw_Y9_11qcW8&prettyPrint=false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = req.post(request_url, headers=headers)\n",
    "data = req.post(request_url, json=payload)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'responseContext': {'visitorData': 'CgtpdEQtRFlRbXNlayiky6iRBg%3D%3D',\n",
       "  'serviceTrackingParams': [{'service': 'CSI',\n",
       "    'params': [{'key': 'c', 'value': 'WEB'},\n",
       "     {'key': 'cver', 'value': '2.20220309.01.00'},\n",
       "     {'key': 'yt_li', 'value': '0'},\n",
       "     {'key': 'GetTranscriptWatch_rid', 'value': '0xa1537f5ba793dcea'}]},\n",
       "   {'service': 'GFEEDBACK',\n",
       "    'params': [{'key': 'logged_in', 'value': '0'},\n",
       "     {'key': 'e',\n",
       "      'value': '24002022,24120829,24080738,24146324,24175487,24045470,24138064,24120819,24002025,24106839,39321475,24082662,24176755,24180221,24110902,24154586,24169458,24109689,24036948,24161848,24004644,24077241,23998056,24135310,24166867,24145515,23918597,24007246,23930656,24139173,24007790,23744176,23983296,23986028,24034168,24165080,24140247,24138442,23966208,24152443,24141462,23946420,1714251,24182568,23804281,24169726,23853953,24178748,24077266,23882685,24037231,24148481,24154616,24045469,24001373,24085811,24180069,24177193,23934970,24062269'}]},\n",
       "   {'service': 'GUIDED_HELP', 'params': [{'key': 'logged_in', 'value': '0'}]},\n",
       "   {'service': 'ECATCHER',\n",
       "    'params': [{'key': 'client.version', 'value': '2.20220309'},\n",
       "     {'key': 'client.name', 'value': 'WEB'},\n",
       "     {'key': 'client.fexp',\n",
       "      'value': '24002022,24120829,24080738,24146324,24175487,24045470,24138064,24120819,24002025,24106839,39321475,24082662,24176755,24180221,24110902,24154586,24169458,24109689,24036948,24161848,24004644,24077241,23998056,24135310,24166867,24145515,23918597,24007246,23930656,24139173,24007790,23744176,23983296,23986028,24034168,24165080,24140247,24138442,23966208,24152443,24141462,23946420,1714251,24182568,23804281,24169726,23853953,24178748,24077266,23882685,24037231,24148481,24154616,24045469,24001373,24085811,24180069,24177193,23934970,24062269'}]}],\n",
       "  'mainAppWebResponseContext': {'loggedOut': True},\n",
       "  'webResponseContextExtensionData': {'hasDecorated': True}},\n",
       " 'trackingParams': 'CAAQw7wCIhMI4LT25vm79gIVSWyyCh1ZYAli'}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = json.loads(data.content)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_payload(payload, path=None):\n",
    "    from time import sleep\n",
    "    from copy import deepcopy\n",
    "\n",
    "    if path == None:\n",
    "        path = []\n",
    "\n",
    "    removed = {}\n",
    "\n",
    "    for k in list(payload.keys()):\n",
    "        temp_payload = deepcopy(payload)\n",
    "        temp_payload.pop(k, None)\n",
    "        data = req.post(request_url, json=temp_payload)\n",
    "        removed[k] = data.status_code\n",
    "        print(f'removing: {k}, code:{data.status_code}')\n",
    "        sleep(0.5)\n",
    "\n",
    "    for k, v in removed.items():\n",
    "        if v != 200:\n",
    "            for k2 in list(payload[k].keys()):\n",
    "                temp_payload = deepcopy(payload)\n",
    "                temp_payload[k].pop(k2, None)\n",
    "                data = req.post(request_url, json=temp_payload)\n",
    "                can_remove = 'Yes'\n",
    "                try:\n",
    "                    json.loads(data.content)['actions']\n",
    "                except:\n",
    "                    can_remove = 'No'\n",
    "                if isinstance(removed[k], int):\n",
    "                    removed[k] = {}\n",
    "                removed[k][k2] = data.status_code\n",
    "                print(f'removing: {k2} in {k}, code:{data.status_code} can remove: {can_remove}')\n",
    "                sleep(0.5)\n",
    "\n",
    "    for k, v in removed.items():\n",
    "        if isinstance(v, dict):\n",
    "            for k2, v2 in v.items():\n",
    "                if v2 != 200:\n",
    "                    for j in list(payload[k][k2].keys()):\n",
    "                        temp_payload = deepcopy(payload)\n",
    "                        temp_payload[k][k2].pop(j, None)\n",
    "                        data = req.post(request_url, json=temp_payload)\n",
    "                        can_remove = 'Yes'\n",
    "                        try:\n",
    "                            json.loads(data.content)['actions']\n",
    "                        except:\n",
    "                            can_remove = 'No'\n",
    "                        print(f'removing: {j} in {k2} in {k}, code:{data.status_code} can remove: {can_remove}')\n",
    "                        sleep(0.5)\n",
    "\n",
    "# test_payload(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'actions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5808/845967179.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtranscript_json\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'actions'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'updateEngagementPanelAction'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'content'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'transcriptRenderer'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'body'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'transcriptBodyRenderer'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cueGroups'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtranscript_json\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'actions'"
     ]
    }
   ],
   "source": [
    "transcript_json = data['actions'][0]['updateEngagementPanelAction']['content']['transcriptRenderer']['body']['transcriptBodyRenderer']['cueGroups']\n",
    "transcript_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_group(cue_group):\n",
    "    '''\n",
    "    parses cue groups\n",
    "\n",
    "    returns: (timestamp, text)\n",
    "    '''\n",
    "    return cue_group['transcriptCueGroupRenderer']['formattedStartOffset']['simpleText'], cue_group['transcriptCueGroupRenderer']['cues'][0]['transcriptCueRenderer']['cue']['simpleText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'00:00': \"what i've got here is something\",\n",
       " '00:03': \"extra special it's an rtx 3090 that can\",\n",
       " '00:06': 'be yours all you need to do to be',\n",
       " '00:07': 'eligible is to sign up to gtc and attend',\n",
       " '00:11': 'at least one session using my url which',\n",
       " '00:13': 'i will put in the description but this',\n",
       " '00:16': 'is not just any old rtx 3090 this is an',\n",
       " '00:20': 'rtx 3090 signed by the leather jacketed',\n",
       " '00:23': 'ceo and founder of nvidia jensen himself',\n",
       " '00:26': 'also through some highly advanced',\n",
       " '00:28': 'analysis of some of the promotional',\n",
       " '00:31': 'materials that nvidia sent over for this',\n",
       " '00:34': \"uh i do believe that there there's first\",\n",
       " '00:36': \"of all there's only one photo of jensen\",\n",
       " '00:38': 'actually holding one of these signed',\n",
       " '00:40': 'gpus',\n",
       " '00:41': 'and each signature on the box does vary',\n",
       " '00:44': 'a little bit right he actually manually',\n",
       " '00:46': \"signed all of them um and there's just\",\n",
       " '00:48': 'not that many of them to begin with but',\n",
       " '00:50': \"the one that is in the photo that he's\",\n",
       " '00:52': 'holding up',\n",
       " '00:54': 'is',\n",
       " '00:55': 'i believe that exact one it matches up',\n",
       " '00:57': 'perfectly so so not only do you have the',\n",
       " '01:00': 'opportunity to win that signed gpu now',\n",
       " '01:02': 'when you frame this on your wall you can',\n",
       " '01:04': 'print out a photo of jensen actually',\n",
       " '01:06': 'holding your signed gpu and actually put',\n",
       " '01:09': 'that up probably frame that as well on',\n",
       " '01:11': \"your wall so that's just that's super\",\n",
       " '01:13': 'cool so uh on to probably the most',\n",
       " '01:17': 'frequently asked question on the channel',\n",
       " '01:19': 'recently at least and that is where is',\n",
       " '01:22': 'neural networks from scratch part 10.',\n",
       " '01:25': 'and before that it was part six and',\n",
       " '01:27': \"probably in the future it'll be some\",\n",
       " '01:28': 'other part since they seem to come out',\n",
       " '01:30': 'in chunks',\n",
       " '01:31': \"the deal is it's been like two years uh\",\n",
       " '01:34': 'just full steam ahead 100 or nearly 100',\n",
       " '01:37': 'on neural networks from scratch and',\n",
       " '01:40': 'the end result is in my opinion the best',\n",
       " '01:43': 'content from the channel hands down the',\n",
       " '01:46': \"book is incredible the videos that we've\",\n",
       " '01:47': 'released so far are incredible',\n",
       " '01:50': 'but this took a ton of work and because',\n",
       " '01:52': 'of that i had to basically not do a',\n",
       " '01:55': 'whole bunch of everything else so',\n",
       " '01:57': 'since the book had been complete and',\n",
       " '01:59': 'definitely put out a few more neural',\n",
       " '02:01': 'networks from scratch videos',\n",
       " '02:03': \"i've just been wanting to get into other\",\n",
       " '02:05': 'stuff besides just neural networks from',\n",
       " '02:06': \"scratch that's just like i said it's\",\n",
       " '02:08': \"been like all we've been doing and in\",\n",
       " '02:10': 'the meantime everything else is taking a',\n",
       " '02:12': 'back seat so for example um a few of the',\n",
       " '02:15': 'things that just happened to become',\n",
       " '02:17': 'available and opportunities that have',\n",
       " '02:18': 'arisen um are like behind me here this',\n",
       " '02:21': 'camino machine an incredible machine but',\n",
       " '02:24': 'if that opportunity had arisen while we',\n",
       " '02:27': 'were doing the neural networks from',\n",
       " '02:28': 'scratch book i could not have done it',\n",
       " '02:31': \"but luckily it came up about i don't\",\n",
       " '02:33': 'know uh towards the end of last year',\n",
       " '02:35': 'basically that this started to form and',\n",
       " '02:38': \"i'm very thankful that we're able to do\",\n",
       " '02:40': 'a project with this but that project',\n",
       " '02:42': \"that we're planning with the camino\",\n",
       " '02:43': 'machine',\n",
       " '02:44': 'is',\n",
       " '02:45': \"a massive undertaking so it's just it\",\n",
       " '02:48': 'requires our full attention and',\n",
       " '02:49': \"especially right now primarily daniel's\",\n",
       " '02:51': 'attention um is kind of getting us',\n",
       " '02:53': 'prepared for this so',\n",
       " '02:55': 'um the the neural networks from scratch',\n",
       " '02:57': 'videos take about like two weeks of our',\n",
       " '03:00': 'full attention and and if not two at',\n",
       " '03:03': 'least one but for sure like p10 is a is',\n",
       " '03:06': 'a big one and like p10 probably 11 and',\n",
       " '03:08': '12 are all like very very big production',\n",
       " '03:11': 'um going to be very challenging and also',\n",
       " '03:14': 'those videos i know why you guys love',\n",
       " '03:15': 'them because they take so much effort on',\n",
       " '03:18': 'both of our parts and they have to be',\n",
       " '03:20': \"perfect like they're the most\",\n",
       " '03:21': 'overproduced or not overproduced but the',\n",
       " '03:23': 'most produced videos uh that that i put',\n",
       " '03:26': 'out and the',\n",
       " '03:28': 'the content and the subject of them is',\n",
       " '03:30': 'very complex and the goal is to make it',\n",
       " '03:34': 'as simple as possible without you know',\n",
       " '03:35': 'glossing over and part of the problem is',\n",
       " '03:38': 'because of how much time both me and',\n",
       " '03:40': 'daniel have spent with neural networks',\n",
       " '03:42': \"from scratch we've seen this material\",\n",
       " '03:43': 'like 20 plus times um it becomes very',\n",
       " '03:47': 'difficult to not gloss over things so',\n",
       " '03:49': 'you have to give like extra focus and',\n",
       " '03:51': 'then',\n",
       " '03:52': \"i i think we've done a few neural hours\",\n",
       " '03:54': 'from scratch videos without any single',\n",
       " '03:56': 'mistake but',\n",
       " '03:58': 'most of them uh for sure i would say at',\n",
       " '04:01': 'least all of them have some sort of',\n",
       " '04:02': 'mistake that almost gets missed and then',\n",
       " '04:05': 'some of them have you know small errors',\n",
       " '04:07': \"that just get missed because we've seen\",\n",
       " '04:09': 'this a million times',\n",
       " '04:10': 'so um so just keep that in mind like i i',\n",
       " '04:13': 'plan to do more neural networks from',\n",
       " '04:15': 'scratch videos but',\n",
       " '04:17': \"there's just so many other things that i\",\n",
       " '04:18': 'want to do so another thing is like',\n",
       " '04:20': 'those the the quadrupeds so the little',\n",
       " '04:22': \"bittle dog and stuff um i've been\",\n",
       " '04:24': 'wanting to get into reinforcement',\n",
       " '04:25': 'learning for so long and that bittle',\n",
       " '04:28': \"robot dog was like oh that's a perfect\",\n",
       " '04:31': 'thing to to kind of get into doing',\n",
       " '04:33': 'reinforcement learning with and this',\n",
       " '04:35': \"camino here i've the project that we're\",\n",
       " '04:37': 'going to do on that is a project that',\n",
       " '04:39': \"i've been wanting to do for years and so\",\n",
       " '04:43': \"so i'm very happy to be able to do that\",\n",
       " '04:46': 'and i love you guys i love you guys for',\n",
       " '04:47': 'supporting the neural networks from',\n",
       " '04:49': \"scratch series in the books but you're\",\n",
       " '04:51': 'going to probably just be waiting a',\n",
       " '04:52': 'little bit longer',\n",
       " '04:54': 'but if you need the content i mean the',\n",
       " '04:55': \"book does exist it's it's complete\",\n",
       " '04:58': 'all the content is there so uh i know',\n",
       " '05:00': 'some of you guys still waiting on the',\n",
       " '05:01': 'videos also have the books and like the',\n",
       " '05:03': 'video is just another form i get that um',\n",
       " '05:06': \"but you'll have to wait\",\n",
       " '05:07': \"if you don't already have the book you\",\n",
       " '05:08': 'can get it at nnfs.io',\n",
       " '05:10': \"it's 29 for the ebook um which i\",\n",
       " '05:13': 'understand might still be out of range',\n",
       " '05:15': 'for some of you but the the the price is',\n",
       " '05:17': 'like',\n",
       " '05:18': 'uh',\n",
       " '05:19': \"there is no other book i i don't think\",\n",
       " '05:21': 'there is even another book in existence',\n",
       " '05:23': 'that is like this this is not like a',\n",
       " '05:25': 'intro to machine learning or a basics',\n",
       " '05:27': \"with pytorch it's not that it's neural\",\n",
       " '05:29': 'networks from scratch and python all the',\n",
       " '05:31': 'calculus everything is is being shown',\n",
       " '05:34': \"short of like a master's or a phd\",\n",
       " '05:35': \"program i don't know where you out where\",\n",
       " '05:37': 'else this material exists at some point',\n",
       " '05:39': \"i'm sure somebody will copy it um\",\n",
       " '05:42': \"but for now this is it and for it's for\",\n",
       " '05:43': '29. 29. so',\n",
       " '05:46': \"so uh i'd say it's a pretty good steal\",\n",
       " '05:48': \"and if you still can't afford 29\",\n",
       " '05:50': \"even though i think that's a good price\",\n",
       " '05:52': 'some people live in countries where the',\n",
       " '05:54': 'exchange rate is just terrible and you',\n",
       " '05:57': \"really shouldn't be paying like a\",\n",
       " '05:58': \"month's salary for the book so if you do\",\n",
       " '06:01': 'have a problem uh affording the ebook i',\n",
       " '06:03': \"can discount that the print book it's\",\n",
       " '06:05': 'over 600 pages full color heavy weighted',\n",
       " '06:07': \"pages um i can't discount the physical\",\n",
       " '06:10': 'book',\n",
       " '06:11': 'they get lost in shipping and stuff',\n",
       " '06:12': 'every that that price is as cheap as it',\n",
       " '06:14': 'gets people ask all the time like are',\n",
       " '06:16': 'there going to be like discounts for the',\n",
       " '06:17': 'holidays and stuff no no never there',\n",
       " '06:19': 'would never be a discount for the',\n",
       " '06:21': 'holiday because that is how much that',\n",
       " '06:22': 'book costs to',\n",
       " '06:24': 'print ship and then account for like',\n",
       " '06:26': \"loss and other issues and shipping you'd\",\n",
       " '06:28': 'be surprised how many neural networks',\n",
       " '06:30': 'from scratch books have just',\n",
       " '06:33': \"disappeared they're all attract no one\",\n",
       " '06:35': 'is like getting the book and then saying',\n",
       " '06:36': \"i didn't get it they just disappear in\",\n",
       " '06:39': 'shipping',\n",
       " '06:40': 'so',\n",
       " '06:41': \"anyway um so that's that on the neural\",\n",
       " '06:43': 'network from scratch p10 i get it i know',\n",
       " '06:45': 'you guys want it i love that content too',\n",
       " '06:47': 'but there are other things that i also',\n",
       " '06:49': 'love and that i want to work on so',\n",
       " '06:51': 'besides the reinforcement learning stuff',\n",
       " '06:53': 'so we got reinforcement with the biddle',\n",
       " '06:55': 'reinforcement learning with the bitter',\n",
       " '06:56': \"robot dog coming as well as i'm doing\",\n",
       " '06:59': 'reinforcement learning with starcraft 2',\n",
       " '07:01': 'or at least going to try starcraft 2 is',\n",
       " '07:03': 'really challenging because',\n",
       " '07:05': 'you have to simplify the problem because',\n",
       " '07:07': 'the you know default observation space',\n",
       " '07:10': 'and the default action space of',\n",
       " '07:12': 'something like starcraft 2',\n",
       " '07:14': 'is',\n",
       " '07:16': \"i don't even know what the number is but\",\n",
       " '07:18': 'probably in the millions for both so um',\n",
       " '07:20': \"maybe the observation space isn't that\",\n",
       " '07:22': 'complex but for sure like the plausible',\n",
       " '07:24': 'action space is',\n",
       " '07:26': 'massive so',\n",
       " '07:28': 'so um so yeah that was just a fun',\n",
       " '07:30': 'challenge but',\n",
       " '07:31': \"so i've got the starcraft 2 ai and\",\n",
       " '07:33': 'reinforcement learning reinforcement',\n",
       " '07:35': 'learning with biddle hopefully irl like',\n",
       " '07:37': 'in real life physical',\n",
       " '07:38': 'um',\n",
       " '07:40': \"the camino with the project that i'm not\",\n",
       " '07:41': 'yet going to announce but hopefully very',\n",
       " '07:43': 'soon',\n",
       " '07:44': \"uh and then finally like there's so much\",\n",
       " '07:46': 'stuff that just never makes it to a',\n",
       " '07:48': 'video even though i really wanted to do',\n",
       " '07:50': 'something with it',\n",
       " '07:51': 'and the two things that i want to bring',\n",
       " '07:52': 'up here are neo x from a luther ai',\n",
       " '07:57': 'they just recently i think it was like a',\n",
       " '07:59': \"few weeks ago possibly i don't know time\",\n",
       " '08:01': 'is flying right now um they released neo',\n",
       " '08:04': 'x which is a 20 billion parameter',\n",
       " '08:06': \"transformer model which isn't nearly the\",\n",
       " '08:08': \"biggest it's quite large but not not\",\n",
       " '08:10': \"necessarily the biggest model we've ever\",\n",
       " '08:12': 'heard of but this is the largest',\n",
       " '08:15': 'general purpose transformer model that',\n",
       " '08:17': 'you can actually just go and download',\n",
       " '08:20': 'the weights and biases of the model from',\n",
       " '08:22': 'there you can fine-tune it for whatever',\n",
       " '08:24': 'your needs are or you can just run that',\n",
       " '08:26': 'model or even cooler you can like do',\n",
       " '08:28': \"research on that model so we don't\",\n",
       " '08:31': 'really have that much really great',\n",
       " '08:32': 'research on like how transformers work',\n",
       " '08:35': \"what's going on as data passes through\",\n",
       " '08:36': \"them or even what's going on as they're\",\n",
       " '08:38': 'being trained very very few people have',\n",
       " '08:42': 'the knowledge the expertise but also the',\n",
       " '08:45': 'ability to experience um',\n",
       " '08:48': 'that and so for people like',\n",
       " '08:51': \"luther elia there i don't really know\",\n",
       " '08:53': 'how to pronounce it but for people like',\n",
       " '08:55': 'that to just',\n",
       " '08:56': 'this massive investment goes on goes',\n",
       " '08:58': 'into it um both money and huge amounts',\n",
       " '09:02': 'of time and just energy goes into this',\n",
       " '09:05': 'and then they just release it um i just',\n",
       " '09:07': \"think that's that's really cool uh so\",\n",
       " '09:09': \"definitely if you're interested in that\",\n",
       " '09:10': \"kind of stuff or if you're looking for\",\n",
       " '09:11': 'something to do like a thesis on uh',\n",
       " '09:13': 'definitely check that out and then also',\n",
       " '09:16': \"there is j1 and there's a few variations\",\n",
       " '09:19': 'but j1 jumbo is 178 billion parameters',\n",
       " '09:23': \"it is not open source uh and you can't\",\n",
       " '09:25': 'like just download it or anything um',\n",
       " '09:28': 'where but i will say even with neox i',\n",
       " '09:30': 'believe not to train it because even',\n",
       " '09:32': 'training it would require way more than',\n",
       " '09:34': 'this but to just run inference on half',\n",
       " '09:37': \"precision i believe you're going to need\",\n",
       " '09:40': 'one of the your i think you need like',\n",
       " '09:42': 'more than 45 gigs of vram which',\n",
       " '09:44': 'basically means you need one of the 48',\n",
       " '09:46': \"gigabyte cards uh so i don't even think\",\n",
       " '09:48': 'like a single a 140 gigabyte will fit it',\n",
       " '09:51': 'you will need an a6000 48 gigabytes or',\n",
       " '09:54': \"you're going to need an rtx 8000 48\",\n",
       " '09:56': 'gigabytes so um',\n",
       " '09:59': 'even that model is very tough to run but',\n",
       " '10:01': 'j1 jumbo at 178 billion parameters',\n",
       " '10:03': \"you're going to have even harder time\",\n",
       " '10:04': 'running that one anyways even if you',\n",
       " '10:06': 'could download that model but aside from',\n",
       " '10:08': 'just simply having like a web prompt',\n",
       " '10:10': 'that you can of course use they also',\n",
       " '10:12': 'have a really simple to use python api',\n",
       " '10:15': 'that you can actually query this model',\n",
       " '10:16': \"and there is a free tier it you'll\",\n",
       " '10:18': 'exhaust it pretty quickly but even the',\n",
       " '10:20': 'paid stuff is pretty cheap so if you',\n",
       " '10:22': 'found something that was kind of cool uh',\n",
       " '10:24': 'you could you could pay for it and',\n",
       " '10:26': \"i think their whole point is they're\",\n",
       " '10:27': 'trying to people like with these large',\n",
       " '10:29': 'models are just trying to figure out',\n",
       " '10:31': 'like how do we',\n",
       " '10:32': 'make these models into something that',\n",
       " '10:34': 'you could use in business and actually',\n",
       " '10:35': 'like sell and make a profit on so',\n",
       " '10:38': 'and something',\n",
       " '10:40': 'similar to that is something like github',\n",
       " '10:42': \"copilot where right now it is free it's\",\n",
       " '10:44': 'in beta or whatever but i would pay',\n",
       " '10:47': \"i don't hopefully no one from github is\",\n",
       " '10:48': 'listening but i would pay an exorbitant',\n",
       " '10:49': \"amount of money to keep using that it's\",\n",
       " '10:51': 'the greatest thing in the world so',\n",
       " '10:53': \"so so um just no i won't pay anything\",\n",
       " '10:56': \"for it make sure it's free or like a\",\n",
       " '10:57': 'dollar a month so',\n",
       " '10:59': \"so anyways um i'm excited to see what\",\n",
       " '11:01': 'the future holds and like i said like',\n",
       " '11:02': \"even even with j1 jumbo i think there's\",\n",
       " '11:04': 'so much',\n",
       " '11:06': 'room for um experimentation and just',\n",
       " '11:09': 'analysis and research with these very',\n",
       " '11:11': 'large transformer models and i promise',\n",
       " '11:13': 'you there are jobs waiting for you at',\n",
       " '11:16': 'the end of that tunnel because',\n",
       " '11:17': \"everyone's trying to figure out like how\",\n",
       " '11:19': 'can we monetize this this technology',\n",
       " '11:21': \"because it's like it's almost de facto\",\n",
       " '11:23': \"technology with a moat right there's so\",\n",
       " '11:25': 'few companies and people that can make',\n",
       " '11:27': 'in like deploy this kind of technology',\n",
       " '11:29': 'that',\n",
       " '11:30': 'businesses are like frothing at the',\n",
       " '11:31': 'mouth to to figure out how how can we',\n",
       " '11:33': 'monetize this',\n",
       " '11:35': \"so anyway um that's uh that's a\",\n",
       " '11:38': \"neurologist from scratch part 10 that's\",\n",
       " '11:39': 'some of the future content that you can',\n",
       " '11:41': 'expect but also there could be totally',\n",
       " '11:42': 'new things that come up uh things that',\n",
       " '11:44': \"don't come out or who knows you know\",\n",
       " '11:46': 'like i might fail at starcraft 2',\n",
       " '11:47': 'reinforcement learning or something like',\n",
       " '11:49': \"that so don't hold your breath\",\n",
       " '11:50': \"so anyways that's all as a reminder\",\n",
       " '11:53': \"don't forget to sign up for gtc using\",\n",
       " '11:56': 'the link in the description you have to',\n",
       " '11:57': 'use that link uh and then all you have',\n",
       " '11:59': 'to do is sign up for that and attend at',\n",
       " '12:00': 'least one uh gtc session which should',\n",
       " '12:03': \"not be a problem they're at all times\",\n",
       " '12:05': 'like some of them are like in the middle',\n",
       " '12:06': 'of the night for me or during the day',\n",
       " '12:08': \"for me or there's all day long basically\",\n",
       " '12:10': 'from the 21st to the i want to say 24th',\n",
       " '12:13': 'so you should have no problem finding a',\n",
       " '12:15': 'session that you can attend and want to',\n",
       " '12:17': \"attend there's topics unlike\",\n",
       " '12:19': \"everything so it's it's definitely a\",\n",
       " '12:21': \"really cool conference and it's all\",\n",
       " '12:22': \"online it's all free um and you have the\",\n",
       " '12:24': 'opportunity to win a signed 30-90 gpu',\n",
       " '12:28': 'from jensen himself which is just super',\n",
       " '12:31': \"cool i'm super jealous of whoever wins\",\n",
       " '12:32': 'that',\n",
       " '12:33': 'anyways that is all for now i will see',\n",
       " '12:36': 'you all in another video',\n",
       " '13:12': 'you'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript = {}\n",
    "for cue_group in transcript_json:\n",
    "    time, text = parse_group(cue_group)\n",
    "    transcript[time] = text\n",
    "transcript\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"what i've got here is something\",\n",
       " \"extra special it's an rtx 3090 that can\",\n",
       " 'be yours all you need to do to be',\n",
       " 'eligible is to sign up to gtc and attend',\n",
       " 'at least one session using my url which',\n",
       " 'i will put in the description but this',\n",
       " 'is not just any old rtx 3090 this is an',\n",
       " 'rtx 3090 signed by the leather jacketed',\n",
       " 'ceo and founder of nvidia jensen himself',\n",
       " 'also through some highly advanced',\n",
       " 'analysis of some of the promotional',\n",
       " 'materials that nvidia sent over for this',\n",
       " \"uh i do believe that there there's first\",\n",
       " \"of all there's only one photo of jensen\",\n",
       " 'actually holding one of these signed',\n",
       " 'gpus',\n",
       " 'and each signature on the box does vary',\n",
       " 'a little bit right he actually manually',\n",
       " \"signed all of them um and there's just\",\n",
       " 'not that many of them to begin with but',\n",
       " \"the one that is in the photo that he's\",\n",
       " 'holding up',\n",
       " 'is',\n",
       " 'i believe that exact one it matches up',\n",
       " 'perfectly so so not only do you have the',\n",
       " 'opportunity to win that signed gpu now',\n",
       " 'when you frame this on your wall you can',\n",
       " 'print out a photo of jensen actually',\n",
       " 'holding your signed gpu and actually put',\n",
       " 'that up probably frame that as well on',\n",
       " \"your wall so that's just that's super\",\n",
       " 'cool so uh on to probably the most',\n",
       " 'frequently asked question on the channel',\n",
       " 'recently at least and that is where is',\n",
       " 'neural networks from scratch part 10.',\n",
       " 'and before that it was part six and',\n",
       " \"probably in the future it'll be some\",\n",
       " 'other part since they seem to come out',\n",
       " 'in chunks',\n",
       " \"the deal is it's been like two years uh\",\n",
       " 'just full steam ahead 100 or nearly 100',\n",
       " 'on neural networks from scratch and',\n",
       " 'the end result is in my opinion the best',\n",
       " 'content from the channel hands down the',\n",
       " \"book is incredible the videos that we've\",\n",
       " 'released so far are incredible',\n",
       " 'but this took a ton of work and because',\n",
       " 'of that i had to basically not do a',\n",
       " 'whole bunch of everything else so',\n",
       " 'since the book had been complete and',\n",
       " 'definitely put out a few more neural',\n",
       " 'networks from scratch videos',\n",
       " \"i've just been wanting to get into other\",\n",
       " 'stuff besides just neural networks from',\n",
       " \"scratch that's just like i said it's\",\n",
       " \"been like all we've been doing and in\",\n",
       " 'the meantime everything else is taking a',\n",
       " 'back seat so for example um a few of the',\n",
       " 'things that just happened to become',\n",
       " 'available and opportunities that have',\n",
       " 'arisen um are like behind me here this',\n",
       " 'camino machine an incredible machine but',\n",
       " 'if that opportunity had arisen while we',\n",
       " 'were doing the neural networks from',\n",
       " 'scratch book i could not have done it',\n",
       " \"but luckily it came up about i don't\",\n",
       " 'know uh towards the end of last year',\n",
       " 'basically that this started to form and',\n",
       " \"i'm very thankful that we're able to do\",\n",
       " 'a project with this but that project',\n",
       " \"that we're planning with the camino\",\n",
       " 'machine',\n",
       " 'is',\n",
       " \"a massive undertaking so it's just it\",\n",
       " 'requires our full attention and',\n",
       " \"especially right now primarily daniel's\",\n",
       " 'attention um is kind of getting us',\n",
       " 'prepared for this so',\n",
       " 'um the the neural networks from scratch',\n",
       " 'videos take about like two weeks of our',\n",
       " 'full attention and and if not two at',\n",
       " 'least one but for sure like p10 is a is',\n",
       " 'a big one and like p10 probably 11 and',\n",
       " '12 are all like very very big production',\n",
       " 'um going to be very challenging and also',\n",
       " 'those videos i know why you guys love',\n",
       " 'them because they take so much effort on',\n",
       " 'both of our parts and they have to be',\n",
       " \"perfect like they're the most\",\n",
       " 'overproduced or not overproduced but the',\n",
       " 'most produced videos uh that that i put',\n",
       " 'out and the',\n",
       " 'the content and the subject of them is',\n",
       " 'very complex and the goal is to make it',\n",
       " 'as simple as possible without you know',\n",
       " 'glossing over and part of the problem is',\n",
       " 'because of how much time both me and',\n",
       " 'daniel have spent with neural networks',\n",
       " \"from scratch we've seen this material\",\n",
       " 'like 20 plus times um it becomes very',\n",
       " 'difficult to not gloss over things so',\n",
       " 'you have to give like extra focus and',\n",
       " 'then',\n",
       " \"i i think we've done a few neural hours\",\n",
       " 'from scratch videos without any single',\n",
       " 'mistake but',\n",
       " 'most of them uh for sure i would say at',\n",
       " 'least all of them have some sort of',\n",
       " 'mistake that almost gets missed and then',\n",
       " 'some of them have you know small errors',\n",
       " \"that just get missed because we've seen\",\n",
       " 'this a million times',\n",
       " 'so um so just keep that in mind like i i',\n",
       " 'plan to do more neural networks from',\n",
       " 'scratch videos but',\n",
       " \"there's just so many other things that i\",\n",
       " 'want to do so another thing is like',\n",
       " 'those the the quadrupeds so the little',\n",
       " \"bittle dog and stuff um i've been\",\n",
       " 'wanting to get into reinforcement',\n",
       " 'learning for so long and that bittle',\n",
       " \"robot dog was like oh that's a perfect\",\n",
       " 'thing to to kind of get into doing',\n",
       " 'reinforcement learning with and this',\n",
       " \"camino here i've the project that we're\",\n",
       " 'going to do on that is a project that',\n",
       " \"i've been wanting to do for years and so\",\n",
       " \"so i'm very happy to be able to do that\",\n",
       " 'and i love you guys i love you guys for',\n",
       " 'supporting the neural networks from',\n",
       " \"scratch series in the books but you're\",\n",
       " 'going to probably just be waiting a',\n",
       " 'little bit longer',\n",
       " 'but if you need the content i mean the',\n",
       " \"book does exist it's it's complete\",\n",
       " 'all the content is there so uh i know',\n",
       " 'some of you guys still waiting on the',\n",
       " 'videos also have the books and like the',\n",
       " 'video is just another form i get that um',\n",
       " \"but you'll have to wait\",\n",
       " \"if you don't already have the book you\",\n",
       " 'can get it at nnfs.io',\n",
       " \"it's 29 for the ebook um which i\",\n",
       " 'understand might still be out of range',\n",
       " 'for some of you but the the the price is',\n",
       " 'like',\n",
       " 'uh',\n",
       " \"there is no other book i i don't think\",\n",
       " 'there is even another book in existence',\n",
       " 'that is like this this is not like a',\n",
       " 'intro to machine learning or a basics',\n",
       " \"with pytorch it's not that it's neural\",\n",
       " 'networks from scratch and python all the',\n",
       " 'calculus everything is is being shown',\n",
       " \"short of like a master's or a phd\",\n",
       " \"program i don't know where you out where\",\n",
       " 'else this material exists at some point',\n",
       " \"i'm sure somebody will copy it um\",\n",
       " \"but for now this is it and for it's for\",\n",
       " '29. 29. so',\n",
       " \"so uh i'd say it's a pretty good steal\",\n",
       " \"and if you still can't afford 29\",\n",
       " \"even though i think that's a good price\",\n",
       " 'some people live in countries where the',\n",
       " 'exchange rate is just terrible and you',\n",
       " \"really shouldn't be paying like a\",\n",
       " \"month's salary for the book so if you do\",\n",
       " 'have a problem uh affording the ebook i',\n",
       " \"can discount that the print book it's\",\n",
       " 'over 600 pages full color heavy weighted',\n",
       " \"pages um i can't discount the physical\",\n",
       " 'book',\n",
       " 'they get lost in shipping and stuff',\n",
       " 'every that that price is as cheap as it',\n",
       " 'gets people ask all the time like are',\n",
       " 'there going to be like discounts for the',\n",
       " 'holidays and stuff no no never there',\n",
       " 'would never be a discount for the',\n",
       " 'holiday because that is how much that',\n",
       " 'book costs to',\n",
       " 'print ship and then account for like',\n",
       " \"loss and other issues and shipping you'd\",\n",
       " 'be surprised how many neural networks',\n",
       " 'from scratch books have just',\n",
       " \"disappeared they're all attract no one\",\n",
       " 'is like getting the book and then saying',\n",
       " \"i didn't get it they just disappear in\",\n",
       " 'shipping',\n",
       " 'so',\n",
       " \"anyway um so that's that on the neural\",\n",
       " 'network from scratch p10 i get it i know',\n",
       " 'you guys want it i love that content too',\n",
       " 'but there are other things that i also',\n",
       " 'love and that i want to work on so',\n",
       " 'besides the reinforcement learning stuff',\n",
       " 'so we got reinforcement with the biddle',\n",
       " 'reinforcement learning with the bitter',\n",
       " \"robot dog coming as well as i'm doing\",\n",
       " 'reinforcement learning with starcraft 2',\n",
       " 'or at least going to try starcraft 2 is',\n",
       " 'really challenging because',\n",
       " 'you have to simplify the problem because',\n",
       " 'the you know default observation space',\n",
       " 'and the default action space of',\n",
       " 'something like starcraft 2',\n",
       " 'is',\n",
       " \"i don't even know what the number is but\",\n",
       " 'probably in the millions for both so um',\n",
       " \"maybe the observation space isn't that\",\n",
       " 'complex but for sure like the plausible',\n",
       " 'action space is',\n",
       " 'massive so',\n",
       " 'so um so yeah that was just a fun',\n",
       " 'challenge but',\n",
       " \"so i've got the starcraft 2 ai and\",\n",
       " 'reinforcement learning reinforcement',\n",
       " 'learning with biddle hopefully irl like',\n",
       " 'in real life physical',\n",
       " 'um',\n",
       " \"the camino with the project that i'm not\",\n",
       " 'yet going to announce but hopefully very',\n",
       " 'soon',\n",
       " \"uh and then finally like there's so much\",\n",
       " 'stuff that just never makes it to a',\n",
       " 'video even though i really wanted to do',\n",
       " 'something with it',\n",
       " 'and the two things that i want to bring',\n",
       " 'up here are neo x from a luther ai',\n",
       " 'they just recently i think it was like a',\n",
       " \"few weeks ago possibly i don't know time\",\n",
       " 'is flying right now um they released neo',\n",
       " 'x which is a 20 billion parameter',\n",
       " \"transformer model which isn't nearly the\",\n",
       " \"biggest it's quite large but not not\",\n",
       " \"necessarily the biggest model we've ever\",\n",
       " 'heard of but this is the largest',\n",
       " 'general purpose transformer model that',\n",
       " 'you can actually just go and download',\n",
       " 'the weights and biases of the model from',\n",
       " 'there you can fine-tune it for whatever',\n",
       " 'your needs are or you can just run that',\n",
       " 'model or even cooler you can like do',\n",
       " \"research on that model so we don't\",\n",
       " 'really have that much really great',\n",
       " 'research on like how transformers work',\n",
       " \"what's going on as data passes through\",\n",
       " \"them or even what's going on as they're\",\n",
       " 'being trained very very few people have',\n",
       " 'the knowledge the expertise but also the',\n",
       " 'ability to experience um',\n",
       " 'that and so for people like',\n",
       " \"luther elia there i don't really know\",\n",
       " 'how to pronounce it but for people like',\n",
       " 'that to just',\n",
       " 'this massive investment goes on goes',\n",
       " 'into it um both money and huge amounts',\n",
       " 'of time and just energy goes into this',\n",
       " 'and then they just release it um i just',\n",
       " \"think that's that's really cool uh so\",\n",
       " \"definitely if you're interested in that\",\n",
       " \"kind of stuff or if you're looking for\",\n",
       " 'something to do like a thesis on uh',\n",
       " 'definitely check that out and then also',\n",
       " \"there is j1 and there's a few variations\",\n",
       " 'but j1 jumbo is 178 billion parameters',\n",
       " \"it is not open source uh and you can't\",\n",
       " 'like just download it or anything um',\n",
       " 'where but i will say even with neox i',\n",
       " 'believe not to train it because even',\n",
       " 'training it would require way more than',\n",
       " 'this but to just run inference on half',\n",
       " \"precision i believe you're going to need\",\n",
       " 'one of the your i think you need like',\n",
       " 'more than 45 gigs of vram which',\n",
       " 'basically means you need one of the 48',\n",
       " \"gigabyte cards uh so i don't even think\",\n",
       " 'like a single a 140 gigabyte will fit it',\n",
       " 'you will need an a6000 48 gigabytes or',\n",
       " \"you're going to need an rtx 8000 48\",\n",
       " 'gigabytes so um',\n",
       " 'even that model is very tough to run but',\n",
       " 'j1 jumbo at 178 billion parameters',\n",
       " \"you're going to have even harder time\",\n",
       " 'running that one anyways even if you',\n",
       " 'could download that model but aside from',\n",
       " 'just simply having like a web prompt',\n",
       " 'that you can of course use they also',\n",
       " 'have a really simple to use python api',\n",
       " 'that you can actually query this model',\n",
       " \"and there is a free tier it you'll\",\n",
       " 'exhaust it pretty quickly but even the',\n",
       " 'paid stuff is pretty cheap so if you',\n",
       " 'found something that was kind of cool uh',\n",
       " 'you could you could pay for it and',\n",
       " \"i think their whole point is they're\",\n",
       " 'trying to people like with these large',\n",
       " 'models are just trying to figure out',\n",
       " 'like how do we',\n",
       " 'make these models into something that',\n",
       " 'you could use in business and actually',\n",
       " 'like sell and make a profit on so',\n",
       " 'and something',\n",
       " 'similar to that is something like github',\n",
       " \"copilot where right now it is free it's\",\n",
       " 'in beta or whatever but i would pay',\n",
       " \"i don't hopefully no one from github is\",\n",
       " 'listening but i would pay an exorbitant',\n",
       " \"amount of money to keep using that it's\",\n",
       " 'the greatest thing in the world so',\n",
       " \"so so um just no i won't pay anything\",\n",
       " \"for it make sure it's free or like a\",\n",
       " 'dollar a month so',\n",
       " \"so anyways um i'm excited to see what\",\n",
       " 'the future holds and like i said like',\n",
       " \"even even with j1 jumbo i think there's\",\n",
       " 'so much',\n",
       " 'room for um experimentation and just',\n",
       " 'analysis and research with these very',\n",
       " 'large transformer models and i promise',\n",
       " 'you there are jobs waiting for you at',\n",
       " 'the end of that tunnel because',\n",
       " \"everyone's trying to figure out like how\",\n",
       " 'can we monetize this this technology',\n",
       " \"because it's like it's almost de facto\",\n",
       " \"technology with a moat right there's so\",\n",
       " 'few companies and people that can make',\n",
       " 'in like deploy this kind of technology',\n",
       " 'that',\n",
       " 'businesses are like frothing at the',\n",
       " 'mouth to to figure out how how can we',\n",
       " 'monetize this',\n",
       " \"so anyway um that's uh that's a\",\n",
       " \"neurologist from scratch part 10 that's\",\n",
       " 'some of the future content that you can',\n",
       " 'expect but also there could be totally',\n",
       " 'new things that come up uh things that',\n",
       " \"don't come out or who knows you know\",\n",
       " 'like i might fail at starcraft 2',\n",
       " 'reinforcement learning or something like',\n",
       " \"that so don't hold your breath\",\n",
       " \"so anyways that's all as a reminder\",\n",
       " \"don't forget to sign up for gtc using\",\n",
       " 'the link in the description you have to',\n",
       " 'use that link uh and then all you have',\n",
       " 'to do is sign up for that and attend at',\n",
       " 'least one uh gtc session which should',\n",
       " \"not be a problem they're at all times\",\n",
       " 'like some of them are like in the middle',\n",
       " 'of the night for me or during the day',\n",
       " \"for me or there's all day long basically\",\n",
       " 'from the 21st to the i want to say 24th',\n",
       " 'so you should have no problem finding a',\n",
       " 'session that you can attend and want to',\n",
       " \"attend there's topics unlike\",\n",
       " \"everything so it's it's definitely a\",\n",
       " \"really cool conference and it's all\",\n",
       " \"online it's all free um and you have the\",\n",
       " 'opportunity to win a signed 30-90 gpu',\n",
       " 'from jensen himself which is just super',\n",
       " \"cool i'm super jealous of whoever wins\",\n",
       " 'that',\n",
       " 'anyways that is all for now i will see',\n",
       " 'you all in another video',\n",
       " 'you']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(transcript.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7ad24e368a1321c2aa741c94dbc3594f7fd123d235b5802da3ae2a108ea4b401"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
